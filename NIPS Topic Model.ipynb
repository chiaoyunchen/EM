{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIPS Topic Model using Expectation Maximization (EM)\n",
    "\n",
    "The UCI Machine Learning dataset repository hosts several datasets recording word counts for documents [here](https://archive.ics.uci.edu/ml/datasets/Bag+of+Words). Here we will use the NIPS dataset.\n",
    "\n",
    "It provides (a) a table of word counts per document and (b) a vocabulary list for this dataset at the link.\n",
    "\n",
    "We implement the multinomial mixture of topics model using our own EM clustering code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster to 30 topics, using a simple mixture of multinomial topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import libs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "from math import log\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.misc import logsumexp as LSE\n",
    "\n",
    "# read data\n",
    "D = 1500\n",
    "W = 12419\n",
    "NNZ = 746316\n",
    "J = 30 # number of topics/ clusters\n",
    "CONVERGENCE_THRESHOLD = 0.1\n",
    "SMOOTHING_CONST = 0.0002\n",
    "\n",
    "data = np.loadtxt(r'data/docword.nips.txt', dtype=int, delimiter=' ',skiprows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CSR matrix for optimal performance as the data matrix is sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store data as numpy matrix\n",
    "# we subtract by 1 to make data zero-indexed\n",
    "row = data[:, 0] - 1\n",
    "col = data[:, 1] - 1\n",
    "values = data[:, 2]\n",
    "\n",
    "x = csr_matrix((values, (row, col)), shape=(D, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start with uniform distribution of p and pi.\n",
    "Rather a better way might be to use results from k-means clustering (a variant of EM) to get rough clusters.\n",
    "\n",
    "**Caution**: pi should not have a zero-element. An infinitesimal smoothing must be applied in such situations, else no documents may be assigned to the corresponding topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# p corresponds to probability of word in a topic\n",
    "p = np.ones((J, W))\n",
    "p = 1.0/W * p\n",
    "\n",
    "# pi corresponds to probability that document belongs to topic\n",
    "pi = np.ones(J)\n",
    "pi = 1.0/J * pi\n",
    "\n",
    "def logsumexp(X):\n",
    "    \"\"\"\n",
    "    log sum exp trick for approximation to avoid underflow/ overflow\n",
    "    :param X: data matrix\n",
    "    :return: log sum exp applied to each row of input matrix X\n",
    "    \"\"\"\n",
    "    x_max = X.max(1)\n",
    "    return x_max + np.log(np.exp(X - x_max[:, None]).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *E-Step*, we compute:\n",
    "\n",
    "In the *M-Step*, we compute:\n",
    "\n",
    "We keep a list of expectation value *q* so we can visualize change in q as we iterate through *EM*\n",
    "\n",
    "*Note: There are several ways to check for convergences of EM. We use difference between expectations in consecutive iterations as measure of convergence* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18221473.4858\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-d977c5b45cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0md\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Zubin/anaconda/lib/python3.5/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;31m##############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Zubin/anaconda/lib/python3.5/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         coo_todense(M, N, self.nnz, self.row, self.col, self.data,\n\u001b[0;32m--> 256\u001b[0;31m                     B.ravel('A'), fortran)\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# EM\n",
    "iter_count = 1\n",
    "list_of_q = [sys.maxsize]\n",
    "\n",
    "while True:\n",
    "    # log likelihood\n",
    "    ll = x.dot(np.log(p).T) + np.log(pi)\n",
    "\n",
    "    # calculate w_i,j matrix\n",
    "    w = np.zeros((D, J))\n",
    "    row_max = np.amax(ll, 1)\n",
    "    terms = logsumexp((ll.T - row_max).T)\n",
    "    sub_array = row_max - terms\n",
    "    w = np.array([ll[i,] - sub_array[i] for i in range(ll.shape[0])])    \n",
    "    w = np.exp(w)\n",
    "    \n",
    "    # normalize each row of w to sum to 1\n",
    "    for row_idx in range(w.shape[0]):\n",
    "        w[row_idx,] = w[row_idx,] / np.sum(w[row_idx,])\n",
    "\n",
    "    ### E-Step ###\n",
    "    q = np.sum(ll * w)\n",
    "    print(q)\n",
    "    list_of_q.append(q)\n",
    "    \n",
    "    # check for convergence\n",
    "    if abs(q - list_of_q[-2]) < CONVERGENCE_THRESHOLD:\n",
    "        break\n",
    "    \n",
    "    ### M-Step ###\n",
    "    # update p\n",
    "    \"\"\"\n",
    "    Reference for optimization\n",
    "    Also use CSR Matrix\n",
    "      for(j in seq(topics)){\n",
    "        #Update p's with additive smoothing\n",
    "        top <- colSums(vecs * wijs[,j]) + smoothing_constant\n",
    "        bottom <- sum(rowSums(vecs) * wijs[,j]) + (smoothing_constant * num_vocab_words)\n",
    "        probs[j,] <-top/bottom\n",
    "    }\n",
    "    \"\"\"\n",
    "    for j in range(J):\n",
    "        # reset n, d\n",
    "        n = np.zeros(W)\n",
    "        d = 0.0\n",
    "        \n",
    "        for i in range(D):\n",
    "            X = x.toarray()\n",
    "            n += X[i,] * w[i,j]\n",
    "            d += np.sum(X[i,]) * w[i,j]\n",
    "            \n",
    "        p[j,] = (n + SMOOTHING_CONST)/ (d + (SMOOTHING_CONST * W))\n",
    "          \n",
    "    # update pi\n",
    "    pi = np.sum(w, 0)/ D\n",
    "    \n",
    "    print(\"finished iteration\", iter_count)\n",
    "    iter_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph showing, for each topic, the probability with which the topic is selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table showing, for each topic, the 10 words with the highest probability for that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
